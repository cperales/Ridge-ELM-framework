@Article{Huang2012,
  Title                    = {{Extreme learning machine for regression and multiclass classification}},
  Author                   = {Huang, Guang-Bin and Zhou, Hongming and Ding, Xiaojian and Zhang, Rui},
  Journal                  = {IEEE Transactions on Systems, Man, and Cybernetics. Part B, Cybernetics},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {513--29},
  Volume                   = {42},

  Abstract                 = {Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the "generalized" single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.},
  Doi                      = {10.1109/TSMCB.2011.2168604},
  File                     = {:home/cperales/Dropbox/Art{\'{i}}culos/ELM/ELM-Unified-Learning.pdf:pdf},
  ISBN                     = {1083-4419},
  ISSN                     = {1941-0492},
  Owner                    = {cperales},
  Pmid                     = {21984515},
  Timestamp                = {2018.01.23}
}

@InProceedings{tian2010new,
  Title                    = {A new modeling method based on bagging ELM for day-ahead electricity price prediction},
  Author                   = {Tian, Huixin and Meng, Bo},
  Booktitle                = {Bio-Inspired Computing: Theories and Applications (BIC-TA), 2010 IEEE Fifth International Conference on},
  Year                     = {2010},
  Organization             = {IEEE},
  Pages                    = {1076--1079}
}

@Article{Riccardi2014a,
  Title                    = {{Cost-sensitive AdaBoost algorithm for ordinal regression based on extreme learning machine}},
  Author                   = {Riccardi, Annalisa and Fern{\'{a}}ndez-Navarro, Francisco and Carloni, Sante},
  Journal                  = {IEEE Transactions on Cybernetics},
  Year                     = {2014},
  Number                   = {10},
  Pages                    = {1898--1909},
  Volume                   = {44},

  Abstract                 = {In this paper, the well known stagewise additive modeling using a multiclass exponential (SAMME) boosting algorithm is extended to address problems where there exists a natural order in the targets using a cost-sensitive approach. The proposed ensemble model uses an extreme learning machine (ELM) model as a base classifier (with the Gaussian kernel and the additional regularization parameter). The closed form of the derived weighted least squares problem is provided, and it is employed to estimate analytically the parameters connecting the hidden layer to the output layer at each iteration of the boosting algorithm. Compared to the state-of-the-art boosting algorithms, in particular those using ELM as base classifier, the suggested technique does not require the generation of a new training dataset at each iteration. The adoption of the weighted least squares formulation of the problem has been presented as an unbiased and alternative approach to the already existing ELM boosting techniques. Moreover, the addition of a cost model for weighting the patterns, according to the order of the targets, enables the classifier to tackle ordinal regression problems further. The proposed method has been validated by an experimental study by comparing it with already existing ensemble methods and ELM techniques for ordinal regression, showing competitive results. {\textcopyright} 2014 IEEE.},
  Doi                      = {10.1109/TCYB.2014.2299291},
  File                     = {:home/cperales/Dropbox/Art{\'{i}}culos/Ensemble/AdaBoostOR(ELM){\_}red{\_}neuronal.pdf:pdf;:home/cperales/Dropbox/Art{\'{i}}culos/cross{\_}validation/Cost-Sensitive AdaBoost.pdf:pdf},
  ISBN                     = {2168-2267},
  ISSN                     = {21682267 (ISSN)},
  Keywords                 = {Boosting,Cost-sensitive AdaBoost,Extreme learning machine,Neural networks,Ordinal regression,SAMME algorithm,extreme learning machine,neural networks,ordinal regression},
  Owner                    = {cperales},
  Pmid                     = {25222730},
  Timestamp                = {2018.01.23},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84904892728{\&}partnerID=40{\&}md5=6b35d195ee4adf0348fd3005be45e72c}
}

@Article{Ran2012,
  Title                    = {{Boosting Ridge Extreme Learning Machine}},
  Author                   = {Ran, Yangjun and Sun, Xiaoguang and Sun, Huyuan and Sun, Lijuan and Wang, Xin and {Ran Y, Sun X, Sun H, Sun L}, Wang X},
  Journal                  = {Proceedings - 2012 IEEE Symposium on Robotics and Applications, ISRA 2012},
  Year                     = {2012},
  Pages                    = {881--884},

  Abstract                 = {Extreme Learning Machine (ELM) was proposed recently for efficiently training single-hidden-layer feed forward neural networks (SLFNs). However, this algorithm often requires a large number of hidden nodes and sometimes performs unstable. In this paper, a novel training algorithm called boosting ridge extreme learning machine (BR-ELM) is proposed which can construct stable generalization performance with a compact network with boosting ridge regression. In the experiments, the BR-ELM remains robust to all the test data and performs stable and response fast with much less nodes.},
  Doi                      = {10.1109/ISRA.2012.6219332},
  File                     = {:home/cperales/Dropbox/Art{\'{i}}culos/Ensemble/Boosting ridge ELM.pdf:pdf},
  ISBN                     = {9781467322072},
  Keywords                 = {,Extreme Learning Machine,Neural networks,SLFN,boosting ridge regression}
}

@Article{Wang2010,
  Title                    = {{Negative correlation learning for classification ensembles}},
  Author                   = {Wang, Shuo and Chen, Huanhuan and Yao, Xin},
  Journal                  = {Proceedings of the International Joint Conference on Neural Networks},
  Year                     = {2010},

  Abstract                 = {This paper proposes a new negative correlation learning (NCL) algorithm, called AdaBoost.NC, which uses an ambiguity term derived theoretically for classification ensembles to introduce diversity explicitly. All existing NCL algorithms, such as CELS and NCCD, and their theoretical backgrounds were studied in the regression context. We focus on classification problems in this paper. First, we study the ambiguity decomposition with the 0-1 error function, which is different from the one proposed by Krogh et al.. It is applicable to both binary-class and multi-class problems. Then, to overcome the identified drawbacks of the existing algorithms, AdaBoost.NC is proposed by exploiting the ambiguity term in the decomposition to improve diversity. Comprehensive experiments are performed on a collection of benchmark data sets. The results show AdaBoost.NC is a promising algorithm to solve classification problems, which gives better performance than the standard AdaBoost and NCCD, and consumes much less computation time than CELS.},
  Doi                      = {10.1109/IJCNN.2010.5596702},
  File                     = {:home/cperales/Dropbox/Art{\'{i}}culos/Ensemble/Negative{\_}correlation.pdf:pdf},
  ISBN                     = {9781424469178},
  ISSN                     = {1098-7576},
  Owner                    = {cperales},
  Timestamp                = {2018.01.24}
}

@incollection{Perales2018,
  doi = {10.1007/978-3-319-92639-1_25},
  url = {https://doi.org/10.1007%2F978-3-319-92639-1_25},
  year = 2018,
  publisher = {Springer International Publishing},
  pages = {302--314},
  author = {Carlos Perales-Gonz{\'{a}}lez and Mariano Carbonero-Ruz and David Becerra-Alonso and Francisco Fern{\'{a}}ndez-Navarro},
  title = {A Preliminary Study of Diversity in Extreme Learning Machines Ensembles},
  booktitle = {Lecture Notes in Computer Science}}

